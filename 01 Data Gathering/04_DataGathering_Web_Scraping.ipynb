{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<div id=\"header\">\n",
        "    <p style=\"color:black; text-align:center; font-weight:bold; font-family:Tahoma, sans-serif; font-size:24px;\">\n",
        "        Data Gathering with Web Scraping\n",
        "    </p>\n",
        "</div>"
      ],
      "metadata": {
        "id": "Uy5g7mIjl6Ie"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div style=\"background-color:#bfbfbf; padding:8px; border:2px dotted black; border-radius:8px; font-family:sans-serif; line-height: 1.7em\">\n",
        "Data gathering is the first and one of the most essential steps in the machine learning workflow.\n",
        "\n",
        "Web Scraping is the process of extracting data from websites, allowing us to access structured or unstructured information available on the internet. It is a valuable tool for collecting data that may not be readily available through APIs or public datasets.\n",
        "\n",
        "When performing web scraping to gather data, the general process follows these steps:\n",
        "\n",
        "HTTP Request: A client (such as our Python code) sends an HTTP request to the website's URL.\n",
        "HTML Response: The server returns an HTML response containing the webpage's content.\n",
        "HTML Parsing: The HTML content is parsed using libraries such as BeautifulSoup or lxml, allowing the extraction of specific elements (e.g., text, links, images).\n",
        "Data Structuring: Extracted data is cleaned and organized into a structured format like a pandas DataFrame.\n",
        "Data Cleaning: After scraping, data cleaning involves steps like handling missing values, removing duplicates, and formatting columns for consistency.\n",
        "Example: Gathering Book Details\n",
        "\n",
        "In this notebook, the process of gathering book details such as titles, prices, ratings, and availability from a mock e-commerce website (Books to Scrape) is demonstrated. By scraping the HTML content of the website, the relevant fields are extracted, processed into a structured format using pandas, and then saved as a CSV file for further analysis. This method can be extended to scrape real-world data from other websites, subject to their terms of service.\n",
        "\n",
        "</div>"
      ],
      "metadata": {
        "id": "YA4k1g_2iOUN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "kGK19CuBeZ4y"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_url = \"http://books.toscrape.com/catalogue/page-{}.html\""
      ],
      "metadata": {
        "id": "KvPonvo3cjUv"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataframes = []"
      ],
      "metadata": {
        "id": "jViut4bufWGu"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final = pd.DataFrame()"
      ],
      "metadata": {
        "id": "_b2H2PoScqBy"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loop through the pages\n",
        "for page in range(1, 51):\n",
        "    webpage = requests.get(base_url.format(page)).text\n",
        "    soup = BeautifulSoup(webpage, 'lxml')\n",
        "\n",
        "    books = soup.find_all('article', class_='product_pod')\n",
        "\n",
        "    # Lists to store data for this page\n",
        "    titles = []\n",
        "    prices = []\n",
        "    availability = []\n",
        "    ratings = []\n",
        "\n",
        "    for book in books:\n",
        "        try:\n",
        "            titles.append(book.h3.a['title'])\n",
        "        except:\n",
        "            titles.append(np.nan)\n",
        "\n",
        "        try:\n",
        "            prices.append(book.find('p', class_='price_color').text.strip())\n",
        "        except:\n",
        "            prices.append(np.nan)\n",
        "\n",
        "        try:\n",
        "            availability.append(book.find('p', class_='instock availability').text.strip())\n",
        "        except:\n",
        "            availability.append(np.nan)\n",
        "\n",
        "        try:\n",
        "            rating_class = book.find('p', class_='star-rating')['class']\n",
        "            ratings.append(rating_class[1] if len(rating_class) > 1 else np.nan)\n",
        "        except:\n",
        "            ratings.append(np.nan)\n",
        "\n",
        "    # Create a DataFrame for this page\n",
        "    df = pd.DataFrame({\n",
        "        'Title': titles,\n",
        "        'Price': prices,\n",
        "        'Availability': availability,\n",
        "        'Rating': ratings\n",
        "    })\n",
        "\n",
        "    dataframes.append(df)"
      ],
      "metadata": {
        "id": "2xJ4t4gufdba"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine DataFrames\n",
        "final = pd.concat(dataframes, ignore_index=True)"
      ],
      "metadata": {
        "id": "JvciqDUZf2Ib"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(final.sample(5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SZ8g_K5Sf62y",
        "outputId": "d0cd53ab-5c36-4590-adc3-d88d2c548932"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                 Title    Price Availability  \\\n",
            "164  The 10% Entrepreneur: Live Your Startup Dream ...  Â£27.55     In stock   \n",
            "227                                       Twenty Yawns  Â£22.08     In stock   \n",
            "910         Travels with Charley: In Search of America  Â£57.82     In stock   \n",
            "856                                        Dark Places  Â£23.90     In stock   \n",
            "844               Fifty Shades Freed (Fifty Shades #3)  Â£15.36     In stock   \n",
            "\n",
            "    Rating  \n",
            "164  Three  \n",
            "227    Two  \n",
            "910   Five  \n",
            "856   Five  \n",
            "844   Five  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the final DataFrame to a CSV file\n",
        "final.to_csv('books_to_scrape.csv', index=False, encoding='utf-8')\n",
        "print(\"Data saved to books_to_scrape.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7z_M8F54gCSB",
        "outputId": "47f09d16-e3d2-4ff7-ad2d-9f76f7d76cb1"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data saved to books_to_scrape.csv\n"
          ]
        }
      ]
    }
  ]
}